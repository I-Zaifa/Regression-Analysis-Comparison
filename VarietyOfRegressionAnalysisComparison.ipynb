{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b4941fd",
   "metadata": {},
   "source": [
    "## Analysis of Stock Prices with Macroeconomic Indicators applied to different regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf863b17",
   "metadata": {},
   "source": [
    " Going through a varity of regression analysis techniques using S&P500 index companies and their closing prices as dep variables and all other m.e indicators as indep variables. The data is for 12 years; from 2012 start to 2023 end. (Month by Month average basis)\n",
    "\n",
    " The indicators specifcally chosen excluded the very common ones such as GPD and Inflation etc. to see the effect of the others.\n",
    " \n",
    "The ones used were: \n",
    "Unemployment Rate,\tFederal Funds Rate,\tHousing Starts,\tPersonal Savings Rate,\tAverage Hourly Earnings,\tMoney Supply M1,\tLong-Term Interest Rates,\tAverage Weekly Hours,\tPersonal Consumption Expenditures,\tPersonal Income."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748abd00",
   "metadata": {},
   "source": [
    "#### The main task was to compare the following models: \n",
    "- Linear Regression\n",
    "- Lasso Regression\n",
    "- XGBoost\n",
    "- ARIMA\n",
    "- Random Forest Regressor\n",
    "\n",
    "Results are at the end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4c43943-d186-4eeb-9848-60dac87f6b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7832c09",
   "metadata": {},
   "source": [
    "The Dataset is already mostly preprocessed and missing values have been handled. Check the merging.py file for details on how it was done for combining historicals and balancesheet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17f911eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Stocks_ME_Merged_dataset.csv'\n",
    "dfile =pd.read_csv(file_path)\n",
    "\n",
    "dfile['Date'] = pd.to_datetime(dfile[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38410593",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfile.drop(columns=['Date', 'Average Close'])\n",
    "y = dfile['Average Close']\n",
    "\n",
    "# Declaring the X - Dep and y - Indep variables\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9c322ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d571d4",
   "metadata": {},
   "source": [
    "### Using Linear Regression\n",
    "(most common method, no parameter tuning for this one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "215b973e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - Mean Squared Error: 532286.3592794591\n",
      "Linear Regression - R2_SCORE: 0.4496216702810921\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# Declaring the model\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "model_predictions = model.predict(X_test_scaled) \n",
    "\n",
    "# Evaluation of the model\n",
    "mse = mean_squared_error(y_test, model_predictions)\n",
    "r2 = r2_score(y_test, model_predictions)\n",
    "\n",
    "print(f'Linear Regression - Mean Squared Error: {mse}')\n",
    "print(f'Linear Regression - R2_SCORE: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc134f95",
   "metadata": {},
   "source": [
    "### Using Lasso Regression\n",
    "(adds a penaltity to coefficients when rss is calculated and reduces the least impactful ones to zero to reduce multicollinearity and focus on more important features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbdfe9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Alpha for Lasso: 166.81005372000558\n",
      "Lasso Regression Mean Squared Error: 629775.631061466\n",
      "Lasso Regression R2 Score: 0.34881881927148495\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "lasso = Lasso()\n",
    "parameters = {'alpha': np.logspace(-4,4,10)} # generates 10 values b/w 10^-4 and 10^4\n",
    "lasso_regressor = GridSearchCV(lasso, parameters, cv=2) # evaluates the model on two different subsets of the data. (b/c of limited data)\n",
    "\n",
    "lasso_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_alpha_lasso = lasso_regressor.best_params_['alpha'] # Alpha value is the penalty added each time\n",
    "print(f'Best Alpha for Lasso: {best_alpha_lasso}')\n",
    "\n",
    "Lasso_prediction = lasso_regressor.predict(X_test_scaled)\n",
    "lasso_mse = mean_squared_error(y_test, Lasso_prediction)\n",
    "lasso_r2 = r2_score(y_test, Lasso_prediction)\n",
    "print(f'Lasso Regression Mean Squared Error: {lasso_mse}')\n",
    "print(f'Lasso Regression R2 Score: {lasso_r2}')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97febc9c",
   "metadata": {},
   "source": [
    "### Using XgBoost \n",
    "(Uses Gradient boosting technique, meaning that it learns from the errors of the previous model and updates it to correct/reduce its residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdd21438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost MSE: 950400.6645766628\n",
      "XGBoost R2 Score: 0.017296007657378265\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Creating and inputting\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=1000, random_state=42)\n",
    "xg_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluating and printing the predictions\n",
    "xgb_predictions = xg_reg.predict(X_test_scaled)\n",
    "xgb_mse = mean_squared_error(y_test, xgb_predictions)\n",
    "xgb_r2 = r2_score(y_test, xgb_predictions)\n",
    "\n",
    "print(f\"XGBoost MSE: {xgb_mse}\")\n",
    "print(f\"XGBoost R2 Score: {xgb_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d151deff",
   "metadata": {},
   "source": [
    "### Using ARIMA \n",
    "(Used for time series data. Better - reportedly- to capture time related trends overtime; basically spicy linear regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37802fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA MSE: 4657320.966387032\n",
      "ARIMA R2 Score: -3.815619430704169\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Converting to time series (basically it gives a certain number to each date to work through in an easier manner)\n",
    "df_time_series = pd.DataFrame({'y':y})\n",
    "df_time_series.index = dfile['Date']\n",
    "\n",
    "arima_model = ARIMA(df_time_series, order=(7,0,1)) # pdq\n",
    "# p = how many lagged var's you want to use to predict the next value\n",
    "# d = makes the data stationary; 1 means subtract each obv from previous. *limited data so == 0\n",
    "# q = no. of past errors to include to capture shocks. \n",
    "arima_model_fit = arima_model.fit()\n",
    "\n",
    "# Predictions\n",
    "forecast = arima_model_fit.forecast(steps=len(y_test))\n",
    "arima_mse = mean_squared_error(y_test, forecast)\n",
    "arima_r2 = r2_score(y_test, forecast)\n",
    "\n",
    "print(f\"ARIMA MSE: {arima_mse}\")\n",
    "print(f\"ARIMA R2 Score: {arima_r2}\")\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc063178",
   "metadata": {},
   "source": [
    "### Using Random Forest Regressor\n",
    "(Splits the data, based on random selection of features, into a given number of trees and further branches them out till it hits a leaf node which has variance either minimum than the parent tree or some other stopping condition has been met i.e. maximum number of branch splits. Then, it averages them all out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "443f5460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFR MSE: 703187.7341453121\n",
      "RFR R2_Score: 0.2729114998896135\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Creating the model. I Love this model\n",
    "rfr = RandomForestRegressor(n_estimators=1000)\n",
    "rfr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions and Evaluation\n",
    "rfr_predictions = rfr.predict(X_test_scaled)\n",
    "rfr_mse = mean_squared_error(y_test, rfr_predictions) \n",
    "rfr_r2 = r2_score(y_test, rfr_predictions)\n",
    "\n",
    "print(f\"RFR MSE: {rfr_mse}\")\n",
    "print(f\"RFR R2_Score: {rfr_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1083576",
   "metadata": {},
   "source": [
    "#### The following are the results produced. The data and the M.E indicators chosen can be blamed for the low R2 scores but the point here, once again was not to predict the outcomes but compares them across carious techniques to see which one is the best one when used in the most simple manner without any major fine tuning required.\n",
    "\n",
    "- Linear Regression - Mean Squared Error: 532286.3592794591\n",
    "- Linear Regression - R2_SCORE: 0.4496216702810921\n",
    "#\n",
    "- Lasso Regression Mean Squared Error: 629775.631061466\n",
    "- Lasso Regression R2 Score: 0.34881881927148495\n",
    "#\n",
    "- XGBoost MSE: 950400.6645766628\n",
    "- XGBoost R2 Score: 0.017296007657378265\n",
    "#\n",
    "- ARIMA MSE: 4657320.966387032\n",
    "- ARIMA R2 Score: -3.815619430704169\n",
    "#\n",
    "- RFR MSE: 703187.7341453121\n",
    "- RFR R2_Score: 0.2729114998896135\n",
    "#\n",
    "\n",
    "The clear winner here is Linear Regression which I presume would be to the linearity in the models independant variables and how connected they were. If there was more dimentionality in the model, like other features included than m.e indicators such as financial metrics, then, I think that would have introduced a lot of nonlinearity in the model for which Random Forest would have been best applied to. I am underwhelemed by the performance of Arima model. Perhaps I donot understand its usage well enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdf7d7d",
   "metadata": {},
   "source": [
    "# Valete"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
